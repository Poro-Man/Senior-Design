## Chapter 4.7 — Text Generation (Sampling from GPT)

### Big picture
Up to now (§4.6) we coded the GPT model (`Toilet`) that takes token indices and produces logits `(B, T, V)`. In §4.7 we extend this into **actual text generation**.  
The key idea:  
- Repeatedly feed the model the current context.  
- Sample (or greedily pick) the next token from the output distribution.  
- Append the token to the context and continue.

This process is called **autoregressive generation** and is how GPT creates sequences one token at a time.

---

### Visual intuition
Cycle of autoregressive decoding:

![Cycle Example](cyclce%20example.png)

Illustration of the decode loop:

![Illustration](illustrate.png)

Example of generated output:

![Generated Text](text.png)

And the project file structure tying it together:

![File Structure](811725df-9fa9-4bcf-b664-129be7a8dd8d.png)

---

### Implementation in our repo

The main file here is **`toilet with flush.py`**. It reuses:  
- `AutoBlocks` (transformer block from §4.5, in `optimus.py`)  
- `MultiHeadAttn` (masked self-attention from §4.3, in `Multihead.py`)  
- `ConveyorBelt` (position-wise MLP from §3.6, in `Glue.py`)  
- Config from `gpt_config.py`

---

### The GPT model (same as before, now with generate function)

```python
class Toilet(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.tok_emb = nn.Embedding(cfg["vocab_size"], cfg["emb_dim"])
        self.pos_emb = nn.Embedding(cfg["context_length"], cfg["emb_dim"])
        self.drop_emb = nn.Dropout(cfg["drop_rate"])
        self.trf_blocks = nn.Sequential(
            *[AutoBlocks(cfg) for _ in range(cfg["n_layers"])]
        )
        self.final_norm = nn.LayerNorm(cfg["emb_dim"])
        self.out_head = nn.Linear(cfg["emb_dim"], cfg["vocab_size"], bias=False)

    def forward(self, in_idx):
        B, T = in_idx.size()
        tok_embeds = self.tok_emb(in_idx)
        pos_embeds = self.pos_emb(torch.arange(T, device=in_idx.device))
        x = self.drop_emb(tok_embeds + pos_embeds)
        x = self.trf_blocks(x)
        x = self.final_norm(x)
        logits = self.out_head(x)
        return logits
```

This is essentially the same as in §4.6 — but in §4.7 we add a **sampling loop**.

---

### Generation loop (greedy decoding)

```python
@staticmethod
def generate_text(model, idx, max_new_tokens, context_size):
    model.eval()
    for _ in range(max_new_tokens):
        # crop to context window
        idx_cond = idx[:, -context_size:]
        with torch.no_grad():
            logits = model(idx_cond)

        logits = logits[:, -1, :]                 # last position
        probs = torch.softmax(logits, dim=-1)     # convert to probabilities
        idx_next = torch.argmax(probs, dim=-1, keepdim=True)  # greedy pick
        idx = torch.cat((idx, idx_next), dim=1)   # append token
    return idx
```

- **Context cropping**: ensures we don’t exceed the max sequence length.  
- **Logits from last token**: we only need the distribution for the next step.  
- **Greedy pick**: here we take `argmax`, though real GPT uses sampling + temperature, nucleus top-p, etc.  
- **Concatenation**: append the new token to the sequence.

---

### Example run (`__main__` in flush.py)
With GPT-2 encoding (`tiktoken`):

```python
if __name__ == "__main__":
    GPT_CONFIG_124M = { ... }  # same as before

    enc = tiktoken.get_encoding("gpt2")
    model = Toilet(GPT_CONFIG_124M)

    start_context = "Hello, I am"
    encoded = enc.encode(start_context)
    encoded_tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0)

    out = model.generate_text(
        model=model,
        idx=encoded_tensor,
        max_new_tokens=6,
        context_size=GPT_CONFIG_124M["context_length"]
    )
    decoded_text = enc.decode(out.squeeze(0).tolist())
    print(decoded_text)
```

Output:
